
Convolutional Neural Networks can best be thought of as operating over volumes. Convolutional comes from the fact that we are convolving two signals: element-wise  multiplication and the sum of the filter and the signal. The layers in a Convolutional Neural Net take volumes of activation functions and produce volumes of activations. The intermediates have the spatial dimensions of width, height, and depth. For my neural net, the depth is the number of channels in the image; three for RGB, and one for greyscale. The Conv layer is the core of a CNN. Here, we receive and input volume that is passed through the layer, and filtered. At this point, the input, an image, is convolved with a filter that slides over the image spatially, computing dot products as it does. Filters may be small in these Conv layers, but they will continue until they have spread across the full depth of the input volume. The result is a slice of our input volume. Once filters have successfully slid along all of our input volume, the model produces activation maps of the responses of each filter at every single spatial dimension. If we had an input volume of 32x32, and a filter size of 5x5, then we would receive a 28x28 activation map since there are 28 positions that a 5x5 filter can pass through. After we have computed the input volume with our filter bank, we also have a bank of activation maps. If we had a green filter, for example, this would create a corresponding activation map. Calculating the dot product while using these filters, creating these activation maps, is called a convolutional operation; we receive a new representation of our input volume as a 28x28x6 output space. In between our filters and activation maps, ReLU, and other operations, are also performed. The following layer has requirements, however. The input of the second layer must be 5x5x6 because 6 would be the depth of the input volume. The input/output between layers must match for a Conv Net to run. If we had an input volume of 32x32x3, and we used 6 filters of 5x4 for a 28x28x6 output space, the next layer would have 10 filters receiving inputs of 5x5x6, translating to 24x24x10. In this way, we go from low level features, into high level features and activation maps. If we had a filter that activated on orientation, for example, then after we would be building up a hierarchy of features to create more and more complex responses to our input; we are essentially creating 3D models of higher and higher level abstractions. One of the ways we do this is with the hyperparameter, stride, that determines by how much a filter slides over an input. For stride to be successful, it has to pass through an input volume evenly; if stride could be uneven, or a float, we could be skipping over, ignoring, or losing part of our input volume. Another hyperparameter we can adjust is padding. Padding allows us to pad an image with zeros, allowing for us to stride over an input volume in ways which may not otherwise work. By using padding, we can make our output the same size as our input coming in; this is important because when working with images, we need to preserve our sizes spatially. Furthermore, since it is possible to have a CNN with potentially thousands of layers, it is important to remember that we do not need to shrink our input volume too fast, because this will generate unnecessary loss. We want to keep a fixed size representation, and thatâ€™s why we use padding.

A Conv layer can be summarized as follows: it can accept an input volume of width, height, depth, and takes four hyperparameters-- the number of filters, the spatial input, the stride, and the amount of zero padding. 

It is important to note that in a Conv layer, we cannot use different filter sizes in different layers. The first Conv layer has access to the image, whereas the second only has access to the outputs of the first layer. So, necessarily, the number of filters must be mirrored top to bottom throughought a Convolutional Neural Net. As we slide a filter through an image, we generate weights; and these weights are used by filters across the network. Since activation maps are simply grids of neurons, all looking at their own patch of input volume, they also share parameters. All neurons in each activation maps, share the weights across layers. An activation map in our example would be a 28x28 map of neuron outputs. Each neuron would be connected to a small region in the input. So a 5x5 filter is also a 5x5 receptor filed for each neuron. Spatially, neurons closer to one another want to process similar things. Looking for a horizontal edge toward the center of an image may be just as important as finding a horizontal edge in the corner of a photo. By sharing weights across layers, the neurons are able to generalize better.

The goal with Conv layers is to preserve spatial size. But if we were to use pooling, we would then be reducing spatial size. With pooling, input layers are squished spatially, sequentially, and for each activation map individually. When using pooling, it is not common to use zero padding. This is because, when we choose to use pooling, we are looking for the most important information, to reduce dimensionality, and so it is not necessary, or even desirable, to pad and image to preserve its ratio. Fortunately, even with pooling, Conv layers are still reliable at determining what is important in an image.
