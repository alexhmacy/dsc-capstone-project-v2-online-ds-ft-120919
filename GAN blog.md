Where Autoencoders are great at taking and input volume and returning a latent representation of that volume (potentially with some undercompleting), a Generative Adverserial Network is capable of outputting results that are on par, if not outperforming their input volumes.

Generative Adverserial Networks are also composed of two distinct parts. The first is the generator, that takes in an input volume, and tries to completely recreate the input in the output, or tries to create an output so similar to the input that it cannot easily be identified as fake. The second part of a GAN, the discriminator, is used for the purpose of identifying real data from fake data. Depending on the results of the discriminator, the GAN either learns and updates its outputs to produce better data, or performs well enough to entirely fool the discriminator (the goal). In a GAN, the generator and the discriminator compete against each other, lending the ‘adverserial’ part to the name.

GANs typically work with image data, and output image data. Just like an Autoencoder, a GAN creates latent representations of the input data, and recreates an image based on these representations. In this way, a GAN’s generator is very similar to an Autoencoder’s decoder: using the most important features, either recreates an image based on the input volume. The difference between GANs and Autoencoders, however, fundamentally lies in how each model is trained. During the first phase of training a GAN, the discriminatr, not the generator, is trained. Here, we give the discriminator a set of ground-truth images from a sample of our training set, and a number of fake images. Labels are set to 0 for fake, and 1 for true. The discriminator uses these labels to train itself using binary-crossentropy; measuring the difference between what is predicted, and what is actual in the range 0-1. The second phase of a GAN then trains the generator. It is used to produce fake images, based on the latent representations of its input volume. The generator then sets all of the labels for every image it makes to 1, erroneously, in the attempts to trick the discriminator. At this point, the weights of the discriminator are frozen, because we are not trying to train the discriminator to be better at its job-- we are only trying to make the generator better at producing fake images. Ironically, the generator does this by only using latent representations of the input volume, not actual inputs like the discriminator. Depending on how well the discriminator can differentiate between the classes ultimately affects how well the generator performs. If the discriminator can detect real images from fake 95% of the time, then the generator can learn valuable latent representations of real images at a 95% confidence rate, hypothetically. Whether or not the generator produces fake images that can trick the discriminator 95% of the time is another matter, determined entirely by training the generator alone.

Since Autoencoders can, unfortunately, be susceptible to undercompleting, Generative Adverserial Networks can be used to create more convincing colorizations of input data. This is because the latent representations made by a GAN are meant to be so convincing that they can fool a discriminator. In an Autoencoder, the model is not performing to create a convincing fake, or even a convincing colorization (unless strictly designed, and trained to do so). In an Autoencoder, the goal is simply to create a latent representation of the data; to reduce dimensionality enough to capture the most important features, and return them. There is no litmus test for whether or not the outputs of an Autoencoder are convincing, only measures of loss when it comes to the feature reconstruction of an output of an Autoencoder. The discriminator of a GAN, however, actively works to ensure that not only are the representations made by the generator latent, but also that they are representative of the input volume itself-- or at least convincing enough to be thought to be part of the input volume, from the same sample in the training data. In this way, the accuracy of a reconstructed image can be automated, to a degree, with a Generative Adverserial Network in a way it cannot be automated in an Autoencoder.  To produce convincing images with an Autoencoder involves intense research, convoluted depth, and long training times; for a GAN, the only requirement is that the generator be trained sufficiently to fool a binary classifier, in place of a person.
In this way, GANs may be more helpful to creating representations of input volumes that can be used in other models, for other tasks. Where an Autoencoder may not return data whole enough for object detection, a GAN might.

The process of semantic segmentation, an ulterior goal to my project, involves a model computing class of every pixel in an input volume. Using filters like a CNN, an image is scanned during semantic segmentation so that every pixel in the image can be assigned to a class, such as “tree,” “bird,” “cloud,” etc. When data is condense, dimensionality is lost, and semantic segmentation becomes harder to perform on the results of a CNN, or an Autoencoder. For a GAN, however, the results may be so convincing, or so close to the actual input data, that object detection is still possible, if not made easier by the colorizing of black and white photos. Where a CNN or Autoencoder can be adjusted to “upsample” their latent representations of data using transposed convolutional layers, for example-- stretching an input volume using zero padding and convolutional computations-- this step is negated during GAN training. So, not only are GANs more practical when working with images, they can also create images that are more useful to other models with fewer computations.
