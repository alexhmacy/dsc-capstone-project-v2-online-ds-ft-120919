{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import keras\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, decode_predictions, preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
    "from skimage.io import imsave\n",
    "from keras.layers import MaxPooling2D, Flatten, Dense, Dropout, Activation, InputLayer, BatchNormalization  \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve\n",
    "                    \n",
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from matplotlib.image import imread\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_images = np.load('l/gray_scale.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_images_1 = np.load('ab/ab/ab1.npy')\n",
    "AB_images_2 = np.load('ab/ab/ab2.npy')\n",
    "AB_images_3 = np.load('ab/ab/ab3.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_images_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_images = np.concatenate((AB_images_1, AB_images_2, AB_images_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_L(data):\n",
    "#     L = []\n",
    "#     i = 0\n",
    "#     while i <= 30:\n",
    "#         for img in np.nditer(data[i]):\n",
    "#             img /= 100\n",
    "#         L.append(img)\n",
    "#         i += 1\n",
    "#     L = np.array(L, dtype='float')\n",
    "#     return L\n",
    "\n",
    "# def normalize_ab(data):\n",
    "#     for img in np.nditer(data):\n",
    "#         img = img*(1.0/128)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_norm = normalize_L(L_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AB_norm_1 = normalize_ab(AB_images_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing L_images to make RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_L(L_images, batch_size = 300):\n",
    "    # Create and empty array of given batch size, 224x224, with three channels\n",
    "    # so that the L channel can fall in a tensor that will eventually hold all three\n",
    "    # RGB channels\n",
    "    L_imgs = np.zeros((batch_size, 224, 224, 3))\n",
    "    # fFr every channel in range 2\n",
    "    for i in range(0, 3):\n",
    "        # a new image in the RGB_imgs array will be a greyscale image from L_images\n",
    "        L_imgs[:batch_size, :, :,i] = L_images[:batch_size]\n",
    "    # Return the standardized version of this new array.\n",
    "    return preprocess_input(L_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating RGB targets from L and AB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_RGB(L_images, AB_images, batch_size = 300):\n",
    "    # Create and empty array of the appropriate size to hold\n",
    "    # L and AB images. It will be of batch size, 224x244\n",
    "    # and be composed of three color channels for RGB\n",
    "    RGB_imgs = np.zeros((batch_size, 224, 224, 3))\n",
    "    # The first color channel will be the L in L_images, up to the batch size\n",
    "    RGB_imgs[:, :, :, 0] = L_images[0:batch_size]\n",
    "    # The second /third color channels will be the AB from AB_images\n",
    "    RGB_imgs[:, :, :, 1:] = AB_images[0:batch_size]\n",
    "    # Convert everything to the same file type\n",
    "    RGB_imgs = RGB_imgs.astype(\"uint8\")\n",
    "    # Create a new empty list to hold all of the RGB images\n",
    "    RGB_array = []\n",
    "    # for every image in the range of the batch size, convert LAB to RGB\n",
    "    # and then append to our empty list of imgs_\n",
    "    for i in range(0, batch_size):\n",
    "        RGB_array.append(cv2.cvtColor(RGB_imgs[i], cv2.COLOR_LAB2RGB))\n",
    "    # Turn our new list of RGB images back into an array   \n",
    "    RGB_array = np.array(RGB_array)\n",
    "    return RGB_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_input = prep_L(L_images, batch_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_output = preprocess_input(make_RGB(L_images, AB_images, batch_size = 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(L_images[25], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = imgs_output[25]\n",
    "lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "L, A, B = cv2.split(lab)\n",
    "plt.imshow(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_output[25]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Baseline Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = Sequential()\n",
    "model_simple.add(Conv2D(strides = 1, kernel_size = 3, filters = 12, padding = \"same\", activation = 'relu'))\n",
    "model_simple.add(Conv2D(strides = 1, kernel_size = 3, filters = 12, padding = \"same\", activation = 'relu'))\n",
    "model_simple.add(Conv2DTranspose(strides = 1, kernel_size = 3, filters = 12, padding = \"same\", activation = 'relu'))\n",
    "model_simple.add(Conv2DTranspose(strides = 1, kernel_size = 3, filters = 3, padding = \"same\", activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple.compile(optimizer='adam', loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_simple.fit(imgs_for_input, imgs_for_output, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model_simple.evaluate(imgs_for_input, imgs_for_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Baseline Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_simple.predict(imgs_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(output[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_for_output[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Baseline Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = model_simple.history\n",
    "\n",
    "plt.plot(h.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(h.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Creating a more powerful CNN with greater Feature Maps and Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(224,224,3)))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish model\n",
    "model.compile(optimizer='rmsprop',loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(imgs_for_input, imgs_for_output, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting updated CNN Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(imgs_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(imgs_for_output[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Fully Connected Network with no Upsampling or Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "updated_model = Sequential()\n",
    "updated_model.add(InputLayer(input_shape=(224,224,3)))\n",
    "updated_model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=1))\n",
    "updated_model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "updated_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "updated_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=1))\n",
    "updated_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "updated_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=1))\n",
    "updated_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "updated_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "updated_model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish model\n",
    "updated_model.compile(optimizer='adam',loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopping_cb = keras.callbacks.EarlyStopping(patience=50, monitor='acc',\n",
    "                                                restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updated_model.fit(imgs_for_input, imgs_for_output, epochs = 100, callbacks=[new_stopping_cb] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = updated_model.history\n",
    "\n",
    "plt.plot(h.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(h.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = updated_model.predict(imgs_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_for_output[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-running updated CNN with Accuracy Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_stopping_cb = keras.callbacks.EarlyStopping(patience=500, monitor='acc',\n",
    "                                                restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(imgs_for_input, imgs_for_output, epochs = 1000, callbacks=[acc_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h = model.history\n",
    "\n",
    "plt.plot(h.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(h.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Accuracy CNN Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(imgs_for_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(imgs_for_output[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation and Feature Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_for_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_for_input[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code loads two sample images, then it creates two filters\n",
    "# and applies them to both images; then it displays one of the resulting\n",
    "# feature maps\n",
    "\n",
    "# from sklearn.datasets import load_sample_image\n",
    "# Load sample images\n",
    "# china = load_sample_image('china.jpg')/255\n",
    "# flower = load_sample_image('flower.jpg')/255\n",
    "\n",
    "image = imgs_for_input\n",
    "batch_size, height, width, channels = imgs_for_input.shape\n",
    "\n",
    "# Create two filters\n",
    "filters = np.zeros(shape=(7,7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1 # Vertical Line\n",
    "filters[3, :, :, 1] = 1 # Horizontal Line\n",
    "\n",
    "outputs = tf.nn.conv2d(image, filters, strides=1, padding='SAME')\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.imshow(outputs[0, :, :, 1], cmap='gray') # Plot the first image's 2nd feature map\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.imshow(outputs[0,:,:,0], cmap='gray')\n",
    "\n",
    "# Pixel intensity is represented as a byte from 0 to 255, so we scale these features\n",
    "# simply by dividing by 255. Then we create two 7x7 filters. We apply them both\n",
    "# using the tf.nn.conv2d() function. We use zero padding with a stride of 2.\n",
    "# tf.nn.conv2d() deserves more explanation: images is the input mini-batch of \n",
    "# 4 dimensions. Filters is the set of filters to apply (also a 4d tensor)/\n",
    "# Strides is equal to 1, but it could also be a 1D array with four elements, \n",
    "# wherethe two central elements are the vertical and horizontal strides. The \n",
    "# beginning and trailing 1s could later be used to specify a batch stride (to skip\n",
    "# some instances) and a chennel stride (to skip some of the previous layer's feature\n",
    "# maps or channels). If padding is set to same, the convolutional layer uses zero padding\n",
    "# if necessary. The output size is set to the numberof input neurons dividided by the\n",
    "# stride, rounded up. Zeros are added as evenly around the inputs as needed. When\n",
    "# strides = 1, the layer's outputs will have the same spatial dimensions (width\n",
    "# and height) as its inputs, hence the name same. If set to 'valid' the conv layer\n",
    "# does not use zero padding and may ignor some rows and colums at the bottom and right\n",
    "# of the image, depending on the stride. This means every neuron's receptive field\n",
    "# lies strictly within valid positions inside the input (it does not go out of bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing -- Another Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_img = array_to_img(imgs_for_output[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "woman_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imsave('tech_woman.jpg', imgs_for_output[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imgs_output[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_output[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rgb2lab(1.0/255*image)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = rgb2lab(1.0/255*image)[:,:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y /=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(1, 224, 224, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.reshape(1, 224, 224, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(None, None, 1)))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish model\n",
    "model.compile(optimizer='rmsprop',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=X, \n",
    "    y=Y,\n",
    "    batch_size=1,\n",
    "    epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output *=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output colorizations\n",
    "cur = np.zeros((224,224, 3))\n",
    "cur[:,:,0] = X[0][:,:,0]\n",
    "cur[:,:,1:] = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageDataGenerator and Feature Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_datagen = ImageDataGenerator(rescale=1./100, shear_range=0.2, zoom_range=0.2,\n",
    "                               rotation_range=20, horizontal_flip=True,\n",
    "                               preprocessing_function=preprocess_input)\n",
    "\n",
    "AB_datagen = ImageDataGenerator(rescale=1./128, shear_range=0.2, zoom_range=0.2,\n",
    "                                rotation_range=20, horizontal_flip=True,\n",
    "                                preprocessing_function=preprocess_input)\n",
    "\n",
    "RGB_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2,\n",
    "                                 rotation_range=20, horizontal_flip=True,\n",
    "                                 preprocessing_function=preprocess_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2,\n",
    "                                   rotation_range=20, horizontal_flip=True)\n",
    "\n",
    "output_datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2,\n",
    "                                    rotation_range=20, horizontal_flip=True)\n",
    "\n",
    "datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2,\n",
    "                             rotation_range=20, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs_input.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_full = normalize_L(imgs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_RGB(data):\n",
    "    rgb = []\n",
    "    for img in np.nditer(data):\n",
    "        img = img*(1.0/255)\n",
    "        rgb.append(img)\n",
    "    rgb = np.array(rgb, 'float')\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_full = (imgs_output/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = tf.keras.preprocessing.image.NumpyArrayIterator(imgs_input,imgs_output, datagen, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grey_four_rank = L_images.reshape(25000, 224, 224, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# greyscale_train = L_datagen.flow(grey_four_rank, batch_size=(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ab_train = AB_datagen.flow(AB_images, batch_size=(300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network\n",
    "datagen_model = Sequential()\n",
    "datagen_model.add(InputLayer(input_shape=(224,224,3)))\n",
    "datagen_model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\n",
    "datagen_model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "datagen_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "datagen_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\n",
    "datagen_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "datagen_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\n",
    "datagen_model.add(UpSampling2D((2, 2)))\n",
    "datagen_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "datagen_model.add(UpSampling2D((2, 2)))\n",
    "datagen_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "datagen_model.add(UpSampling2D((2, 2)))\n",
    "datagen_model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_model.compile(optimizer='adam', loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datagen_model.fit(aug_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datagen_output = datagen_model.predict(imgs_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(datagen_output[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model -- Normalizing AB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_L(data):\n",
    "    for img in np.nditer(data):\n",
    "        img = img*(1.0/255)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_images = L_images.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_images = AB_images.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ab(data):\n",
    "    for img in np.nditer(data):\n",
    "        img = img*(1.0/128)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = normalize_ab(AB_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = L_images[25].reshape(1, 224, 224, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = y[25].reshape(1, 224, 224, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN after Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_model = Sequential()\n",
    "aug_model.add(InputLayer(input_shape=(None, None, 1)))\n",
    "aug_model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))\n",
    "aug_model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))\n",
    "aug_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "aug_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))\n",
    "aug_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "aug_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\n",
    "aug_model.add(UpSampling2D((2, 2)))\n",
    "aug_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "aug_model.add(UpSampling2D((2, 2)))\n",
    "aug_model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "aug_model.add(UpSampling2D((2, 2)))\n",
    "aug_model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish model\n",
    "aug_model.compile(optimizer='rmsprop',loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aug_model.fit(train, target, batch_size=1, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_output = aug_model.predict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_output *= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur = np.zeros((224,224,3))\n",
    "cur[:,:,0] = train[0][:,:,0]\n",
    "cur[:,:,1:] = aug_output[0]\n",
    "\n",
    "color_prediction = lab2rgb(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(color_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Betwixt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of manually creating the variables, use the keras.layers.Conv2D layer\n",
    "conv = keras.layers.Conv2d(filters=32, kernel_size=3, strides=1,\n",
    "                           padding='same', activation='relu')\n",
    "# kernel_size specifies the filter size, so 3x3. As always, we can use\n",
    "# cross validation to find the right hyperparameter values, but this is very\n",
    "# time consuming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single CNN to tackle the Fashion MNIST dataset\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation='relu', padding='same',\n",
    "                        input_shape=[28,28,1]),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "# Page 461.\n",
    "\n",
    "# The first layer uses 64 fairly larger filters (7x7) but no stride because the input\n",
    "# images are not very large. Next we have a max pooling layer which uses a pool\n",
    "# size of 2, so it divides each spatial dimension by a factor of 2. The number of\n",
    "# filters increases from 64 to 128 and then to 256 as we move through the model.\n",
    "# It is common practice to double the number of filters after each pooling layer:\n",
    "# since a pooling layer divides each spatial dimension by a factor of 2, we can\n",
    "# afford to increase our filters by the same factor without exploding the number of\n",
    "# parameters. Next is a fully connected network composed of two hidden dense layers\n",
    "# and a dense output layer. Note that we must flatten the inputs, since a dense network\n",
    "# expects a 1D array of features for each instance. We also add two dropout layers\n",
    "# with a dropout rate of 50% to reduce overfitting.\n",
    "                                 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ResNet-34 using a Sequential Model. Pg. 479\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(64,7, stride=2, input_shape=[224,224,3],\n",
    "                              padding='same', use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation('relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'))\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, acitvation='softmax'))\n",
    "\n",
    "# Pretrained Models, ResNet-50 pg. 480  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations and Future Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
