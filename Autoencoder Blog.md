CNNs to Autoencoders

The receptive fields of neurons in a Convolutional Neural Network may overlap, to tile an entire visual field. Where some neurons react to low level features like horizontal lines, others, higher up, focus on the outputs of these lower level neurons. For image recognition, fully connected convolutional layers can only be used for very small images. Fully connected convolutional layers with larger images would create an exponential number of parameters, overly complicated computations. In a Convolutional neural net, neurons shares weights through partially connected layers. Now, this does not mean that neurons in the first layer are connected to every single pixel, only to pixels in their receptive field; and so on and so forth, all the way through the network. This is achieved by processing the input volume in two dimensions, allowing the neurons to match with their corresponding inputs. Much like it is possible to slide filters over an input volume with zero padding, it is possible to connect larger inputs by spacing out the fields the filters slide along. This is called stride. Fortunately, we do not define neurons manually in a Convolutional network. During training, a CNN will automatically learn the most important features after ‘scanning’ and image with its filters. These activations are then stacked upon each other to create more complex patterns for the network to work with.

When we are working with TensorFlow, images are represented as 3D with width, height, and channels. When we use batching to process images in TensorFlow, then the tensor becomes 4D, with batch included as the first shape. When we process a 4D tensor, then the weights generated by this tensor are also 4D.

It’s important to understand how memory works when running a CNN. During forward propagation, RAM use grows exponentially as the weights for every layer are generated, and then stored for backpropagation (when the CNN computes the loss function, addressing gradient descent). But as large as the memory requirements for a CNN may be during training, they are even bigger when the CNN runs backpropagation; all of the values calculated during the forward pass of the network must be remembered, and then used during the computations of backpropagation. Pooling can help make computations easier to handle, by reducing the input volume along layers of the network. This is further emphasized by the fact that pooling layers do not contain weights that must be remembered in the network; instead, pooling only calculates the maximum of the specified layer, or the average of that layer. Stride can also be used to address memory specifications. For example, if we use a stride of 2, the output image made by our input volume is halved when it comes to height, and width.

Of course, these methods reduce the quality of our data, since data is either destroyed, lost, or removed.  A stride of 2 will return an output volume that is four times smaller than the input volume. If we were to use our CNN in a pipeline that would, say, later identify objects in an image using semantic segmentation, then it would be harder, if not impossible, for our program to adequately identify objects, since the input volume would have too much variance in the data to identify objects with any level of certainty.

Since the results of our pipeline can take on different forms, each affected by the processes we run on the input volume in the CNN, neural networks can take on many different forms as well. Where the standard Convolutional Network may consists of only a few repeating convolutional layers, with ReLU  activations, for example, others may implement many layers, maxpooling, strides, and so forth. Regardless of the design, the input volume is reduced along the network as the size of the activation maps becomes larger, and larger. The best way to achieve this is to use filters that are of the appropriate size. Typically speaking, smaller filters are more efficient for scanning an input volume than larger. Their level of specificity is higher, and the computations are easier to calculate, and are just as efficient since so many are made.

Another form of neural network are Autoencoders. Just like how CNNs process their input volume, creating denser and denser representations of the data, while at the same time reducing its form along the network, Autoencoders take in a dense input volume, and return what is called a “latent representation,” of that data. Latent representations have undergone dimensionality reduction to better assist visualization. Autoencoders can use latent representations to detect important features, or to even create new data that looks similar to the input volume.

Strictly speaking, Autoencoders try to copy their inputs, but with substantially lower dimensionality. This forces the Autoencoder to find the best way to represent an input volume, without losing the most important features of the data. In this way, Autoencoders study the input volume, determine what patterns make the data unique, and then recreate the data by using these patterns. This is achieved through the Autoencoders two part form. The first is the encoder that recognizes the patterns in the input volume, followed by the decoder which takes these patterns and then creates a representation of the input volume. Where a CNN will return an output of activation maps, an Autoencoder will return a representation of the data, based on its most important features. The important thing to note when using an Autoencoder is that the input layer and output layer must mirror each other in their number of neurons. The hazard of Autoencoders is that they undercomplete. A representation may be fuzzy, or unsubstantial compared to its input volume. Undercompleting can be addressed, of course, through deeper Autoencoders, and longer training periods. But time and resources must be considered.
